{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_with_label.csv', 'test.csv', 'train.csv', 'sample_submission.csv']"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "import gensim\n",
    "import os\n",
    "from sklearn import preprocessing \n",
    "os.listdir(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((24842, 6), (24843, 5), (24843, 2))"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "sub = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "train.shape, test.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(49685, 6)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df = pd.concat([train, test])\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search, sub\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "df[\"words\"] = df[\"product_name\"].apply(lambda words : [word for word in words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \") if len(word)>0])\n",
    "df[\"bigram\"] = df.words.apply(lambda words: [f'{words[i]} {words[i+1]}' for i in range(len(words)-1)])\n",
    "df[\"parcent\"] = df.words.apply(lambda words:  len([word for word in words if search(r'\\d+\\%', word)]) > 0)\n",
    "df[\"target\"] = df.department_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {\n",
    "    'cluster_0': [2,3,4,7,8,9,14,16],\n",
    "    'cluster_1': [0,11,19],\n",
    "    'cluster_2': [5,12],\n",
    "    'cluster_3': [1,10,17],\n",
    "    'cluster_4': [13,18],\n",
    "    'cluster_5': [6,15,18,20]\n",
    "}\n",
    "\n",
    "for clm, c in clusters.items():\n",
    "    df[clm] = df.target.isin([Id+1 for Id in c]) * df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "counter = defaultdict(Counter)\n",
    "\n",
    "for e in df.itertuples():\n",
    "    counter[e.target].update([word for word in e.bigram if word != \"\"])\n",
    "        \n",
    "keywords = {}\n",
    "\n",
    "for i in range(21):\n",
    "    mc = counter[i].most_common(20)\n",
    "    keywords[i] = [c[0] for c in mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_feature = [f\"keyword_{i}\" for i in range(21)]\n",
    "\n",
    "for i in range(21):\n",
    "    df[f\"keyword_{i}\"] = df.bigram.apply(lambda words : len(set(words) & set(keywords[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "## 訓練済みの単語ベクトルを読み込んで，product_nameに含まれる単語をベクトルに変換して平均を取ることで，各product_idに対して特徴量ベクトルを作成する\n",
    "\n",
    "## gensimで.vecから読み込むときに時間がかかるので，他のnotebookでpickleで保存したものを使用している\n",
    "model = pd.read_pickle(\"../fast-text/fasttext_gensim_model.pkl\") \n",
    "\n",
    "unused_words = Counter()\n",
    "\n",
    "def get_weight(x):\n",
    "    weight = np.zeros(len(x)) + 1\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if x[i] in ['sleep']:\n",
    "            weight[i] *= 100\n",
    "\n",
    "    for i in range(len(x)-1):\n",
    "        bigram = f\"{x[i]} {x[i+1]}\"\n",
    "        if bigram in keywords:\n",
    "            weight[i] *= 100\n",
    "            weight[i+1] *= 100\n",
    "\n",
    "    return weight\n",
    "\n",
    "def to_vec(x, model):\n",
    "\n",
    "    weight = get_weight(x)\n",
    "\n",
    "    v = np.zeros(model.vector_size)\n",
    "    for i, w in enumerate(x):\n",
    "        try:\n",
    "            v += model[w] ## 単語が訓練済みモデルのvocabにあったら\n",
    "        except:\n",
    "            if w != \"\":\n",
    "                unused_words[w] += 1 ## ベクトルが存在しなかった単語をメモ\n",
    "    v = v / (np.sqrt(np.sum(v ** 2)) + 1e-16) ## 長さを1に正規化\n",
    "    return v    \n",
    "\n",
    "vecs = df[\"words\"].apply(lambda x : to_vec(x, model))\n",
    "vecs = np.vstack(vecs)\n",
    "fasttext_pretrain_cols = [f\"fasttext_pretrain_vec{k}\" for k in range(vecs.shape[1])]\n",
    "vec_df = pd.DataFrame(vecs, columns=fasttext_pretrain_cols)\n",
    "df = pd.concat([df, vec_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"words\"] = df.words.apply(lambda words: [converter[word] if word in converter else word for word in words ])\n",
    "unknowns = [item[0] for item in unused_words.most_common(200)]\n",
    "for i, unknown in enumerate(unknowns):\n",
    "    df[f'unknown_{i}'] = df.words.apply(lambda words: len([word for word in words if word==unknown]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f'unknown_{i}' for i in range(100)] + fasttext_pretrain_cols + keywords_feature + [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"] ## 予測に使用する特徴量の名前\n",
    "target = 'target'\n",
    "n_split = 5 ## cross validationのfold数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[~df[target].isna()]\n",
    "test = df[df[target].isna()]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train[features] = scaler.fit_transform(train[features])\n",
    "test[features] = scaler.transform(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--------fold 0-------\n{'logloss': 0.820042484321558, 'f1_micro': 0.7717321313586607}\n--------fold 1-------\n{'logloss': 0.8518544149563924, 'f1_micro': 0.7633612363168062}\n--------fold 2-------\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-475476f634bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m## train LGBM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboosting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dart'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m## predict on valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ykc-2nd/.venv/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    793\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         super(LGBMClassifier, self).fit(X, _y, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    796\u001b[0m                                         \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                         \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ykc-2nd/.venv/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[1;32m    595\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ykc-2nd/.venv/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ykc-2nd/.venv/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## cross validation\n",
    "scores = []\n",
    "val_list = []\n",
    "preds_test = []\n",
    "pred_cluster = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_split, shuffle = True, random_state=42)\n",
    "\n",
    "for i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, train.target)):\n",
    "    print(f\"--------fold {i_fold}-------\")\n",
    "        \n",
    "    ## train data\n",
    "    x_tr = train.loc[train_idx, features]\n",
    "    y_tr = train.loc[train_idx, target]\n",
    "\n",
    "    ## valid data\n",
    "    x_va = train.loc[valid_idx, features]\n",
    "    y_va = train.loc[valid_idx, target]\n",
    "\n",
    "    ## train LGBM model\n",
    "    model = LGBMClassifier(colsample_bytree=0.2, subsample=0.8, class_weight='balanced', n_estimators=1000, learning_rate=0.1, boosting='dart')\n",
    "    model.fit(x_tr, y_tr, )\n",
    "    \n",
    "    ## predict on valid\n",
    "    pred_val = model.predict_proba(x_va)\n",
    "    pred_cls = model.predict(x_va)\n",
    "\n",
    "    ## evaluate\n",
    "    score = {\n",
    "        \"logloss\"  : log_loss(y_va, pred_val),\n",
    "        \"f1_micro\" : f1_score(y_va, pred_cls, average = \"micro\")}\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "    ## predict on test\n",
    "    pred_test = model.predict_proba(test[features])\n",
    "    preds_test.append(pred_test)\n",
    "\n",
    "    probe = pd.DataFrame(pred_val.round(3), index=y_va.index, columns=[f\"probe_{i}\" for i in range(21)])\n",
    "    df_new = df.loc[y_va.index, ['product_name', 'order_rate', 'order_dow_mode', 'order_hour_of_day_mode', 'department_id']]\n",
    "    df_new['label'] = pd.Series(np.argmax(pred_val, axis = 1), index=y_va.index)\n",
    "    val_list.append(pd.concat([df_new, probe], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    logloss  f1_micro\n0  1.049613  0.775408\n1  1.039451  0.780036\n2  1.101121  0.773349\n3  1.069145  0.768720\n4  1.045023  0.778180",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>logloss</th>\n      <th>f1_micro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.049613</td>\n      <td>0.775408</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.039451</td>\n      <td>0.780036</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.101121</td>\n      <td>0.773349</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.069145</td>\n      <td>0.768720</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.045023</td>\n      <td>0.778180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "# マイクロ平均：ラベル全体でF1スコアを計算する\n",
    "# logloss：1を超える?\n",
    "\n",
    "score_df = pd.DataFrame(scores)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(val_list, axis=0).to_csv('../data/train_with_label.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   product_id  department_id\n0       24842             18\n1       24843              6\n2       24844              6\n3       24845              6\n4       24846             12",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>department_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24842</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24843</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24844</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24845</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24846</td>\n      <td>12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "## cvの各foldで計算した予測値の平均を最終的な予測値に\n",
    "pred_test_final = np.array(preds_test).mean(axis = 0)\n",
    "pred_test_final = np.argmax(pred_test_final, axis = 1)\n",
    "\n",
    "sub[\"department_id\"] = pred_test_final\n",
    "sub.to_csv(\"submission.csv\", index = False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594573183786",
   "display_name": "Python 3.8.2 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}